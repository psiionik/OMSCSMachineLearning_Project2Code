{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5eb4a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from array import array\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path  import join\n",
    "import random as rn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mlrose_hiive as mlrose\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5b2557e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in the dataset into a pandas dataframe object.\n",
    "\n",
    "For the following segments the code snippets were retreved from: https://www.kaggle.com/code/anetakovacheva/interpreting-a-music-genre-classifier\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "input_path = 'musicgenre_datafolder'\n",
    "file_path = join(input_path, 'music_genre.csv')\n",
    "\n",
    "music_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "533f6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleaning and Pre-Processing all of the data\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "There are some duplicated data that needs to be cleaned up\n",
    "\"\"\"\n",
    "music_data.duplicated().any()\n",
    "duplicated = music_data.duplicated()\n",
    "music_data[duplicated]\n",
    "music_data.iloc[9999:10006]\n",
    "music_data.drop([10000, 10001, 10002, 10003, 10004], inplace = True)\n",
    "\n",
    "\"\"\"\n",
    "Removing some columns that don't matter or will complicated the training too much\n",
    "\"\"\"\n",
    "music_data.reset_index(inplace = True)\n",
    "music_data = music_data.drop([\"artist_name\", \"index\", \"instance_id\", \"track_name\", \"obtained_date\"], axis = 1)\n",
    "\n",
    "\"\"\"\n",
    "Normalizing the music data such that it removes invalid values for 'tempo' and converts\n",
    "the column values into a float\n",
    "\"\"\"\n",
    "music_data = music_data.drop(music_data[music_data[\"tempo\"] == \"?\"].index)\n",
    "music_data[\"tempo\"] = music_data[\"tempo\"].astype(\"float\")\n",
    "music_data[\"tempo\"] = np.around(music_data[\"tempo\"], decimals = 2)\n",
    "\n",
    "\"\"\"\n",
    "Encoding the columns that are strings with LabelEncoder since this will mess\n",
    "up the algorithms that require numeric values\n",
    "\"\"\"\n",
    "key_encoder = LabelEncoder()\n",
    "mode_encoder = LabelEncoder()\n",
    "music_data[\"key\"] = key_encoder.fit_transform(music_data[\"key\"])\n",
    "music_data[\"mode\"] = mode_encoder.fit_transform(music_data[\"mode\"])\n",
    "\n",
    "\"\"\"\n",
    "Separating out the column features from the music genre label\n",
    "\"\"\"\n",
    "music_features = music_data.drop(\"music_genre\", axis = 1)\n",
    "music_labels = music_data[\"music_genre\"]\n",
    "\n",
    "\"\"\"\n",
    "Scaling the features out into a scale centered around 0 with a standard deviation of 1\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "music_features_scaled = scaler.fit_transform(music_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ad66119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Splitting the data into Training and Testing Data Sets\n",
    "\"\"\"\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    music_features_scaled, music_labels, test_size = 0.1, stratify = music_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6fbc2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.Series.to_numpy(train_labels, copy=True)\n",
    "test_labels = pd.Series.to_numpy(test_labels, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4fc9b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder()\n",
    "\n",
    "y_train_hot = one_hot.fit_transform(train_labels.reshape(-1, 1)).toarray()\n",
    "y_test_hot = one_hot.transform(test_labels.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7b4c9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting up seed values for reproducability\n",
    "\"\"\"\n",
    "starting_seed = 1234\n",
    "seed_values = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    seed_values.append(starting_seed + i)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "# rn.seed(seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b5b2d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(curve=True, hidden_nodes=[100, 50, 10], learning_rate=0.2,\n",
       "              max_iters=50, random_state=1238)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actually training the NN with the mlrose package using the hyperparameters that were found to be \n",
    "desired in A1.\n",
    "\n",
    "This is to recreate the learning / loss curves that were generated in A1 using mlrose.\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters for NN\n",
    "hidden_nodes = [100, 50, 10]\n",
    "activation = 'relu'\n",
    "algorithm = 'random_hill_climb'\n",
    "max_iters = 50\n",
    "bias = True\n",
    "is_classifier = True\n",
    "learning_rate = 0.2\n",
    "early_stopping = False\n",
    "max_attempts = 10\n",
    "clip_max = 5\n",
    "\n",
    "nn_model = mlrose.NeuralNetwork(\n",
    "    hidden_nodes = hidden_nodes,\n",
    "    activation = activation,\n",
    "    algorithm = algorithm,\n",
    "    max_iters = max_iters,\n",
    "    bias = bias,\n",
    "    is_classifier = is_classifier,\n",
    "    learning_rate = learning_rate,\n",
    "    early_stopping = early_stopping,\n",
    "    max_attempts = max_attempts,\n",
    "#     clip_max = clip_max,\n",
    "    random_state = seed_values[4],\n",
    "    curve = True\n",
    ")\n",
    "\n",
    "nn_model.fit(train_features, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "370680c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85511765 -0.21738124 -0.17115633 ...  0.59541652 -0.95995017\n",
      " -0.01869913]\n",
      "[[26.95908818  2.        ]\n",
      " [26.95908818  3.        ]\n",
      " [26.95212381  5.        ]\n",
      " [26.95212381  6.        ]\n",
      " [26.95166506  8.        ]\n",
      " [26.94751858 10.        ]\n",
      " [26.94660823 12.        ]\n",
      " [26.93899775 14.        ]\n",
      " [26.93692076 16.        ]\n",
      " [26.93692076 17.        ]\n",
      " [26.92977313 19.        ]\n",
      " [26.92977313 20.        ]\n",
      " [26.92977313 21.        ]\n",
      " [26.92977313 22.        ]\n",
      " [26.92510125 24.        ]\n",
      " [26.92510125 25.        ]\n",
      " [26.92237365 27.        ]\n",
      " [26.90418063 29.        ]\n",
      " [26.90418063 30.        ]\n",
      " [26.90418063 31.        ]\n",
      " [26.90418063 32.        ]\n",
      " [26.89627471 34.        ]\n",
      " [26.89627471 35.        ]\n",
      " [26.72689221 37.        ]\n",
      " [26.72313138 39.        ]\n",
      " [26.72282999 41.        ]\n",
      " [26.72282999 42.        ]\n",
      " [26.72282999 43.        ]\n",
      " [26.70598782 45.        ]\n",
      " [26.70084147 47.        ]\n",
      " [26.70084147 48.        ]\n",
      " [26.70084147 49.        ]\n",
      " [26.70025528 51.        ]\n",
      " [26.61542248 53.        ]\n",
      " [26.60381046 55.        ]\n",
      " [26.60381046 56.        ]\n",
      " [26.60381046 57.        ]\n",
      " [26.60381046 58.        ]\n",
      " [26.59166122 60.        ]\n",
      " [26.59166122 61.        ]\n",
      " [26.59166122 62.        ]\n",
      " [26.59166122 63.        ]\n",
      " [26.59166122 64.        ]\n",
      " [26.59166122 65.        ]\n",
      " [26.57835102 67.        ]\n",
      " [26.4932053  69.        ]\n",
      " [26.48250533 71.        ]\n",
      " [26.48250533 72.        ]\n",
      " [26.47374903 74.        ]\n",
      " [26.43538251 76.        ]]\n",
      "26.435382505720845\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(nn_model.fitted_weights)\n",
    "print(nn_model.fitness_curve)\n",
    "print(nn_model.loss)\n",
    "print(nn_model.predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "43d98f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11199960511377659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict labels for train set and assess accuracy\n",
    "y_train_pred = nn_model.predict(train_features)\n",
    "\n",
    "y_train_accuracy = accuracy_score(y_train_hot, y_train_pred)\n",
    "print(y_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2297acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_pred[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6ac69dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Got this code from https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook for \n",
    "instructions on how to load data from the MNIST dataset\n",
    "\"\"\"\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "#             img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a49394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verify Reading Dataset via MnistDataloader class\n",
    "#\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = 'mnist_datafolder'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b7cc6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting up seed values for reproducability\n",
    "\"\"\"\n",
    "starting_seed = 1234\n",
    "seed_values = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    seed_values.append(starting_seed + i)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "# rn.seed(seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "efe18955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Need to do some data pre-processing to make it work with the mlrose NN class\n",
    "\"\"\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "y_train_hot = one_hot.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_hot = one_hot.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "55e9ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.fit_transform(y_train.reshape(-1, 1)).toarray()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5cef9d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(curve=True, hidden_nodes=[2], learning_rate=0.2, max_iters=1000,\n",
       "              random_state=1238)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actually training the NN with the mlrose package using the hyperparameters that were found to be \n",
    "desired in A1.\n",
    "\n",
    "This is to recreate the learning / loss curves that were generated in A1 using mlrose.\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters for NN\n",
    "hidden_nodes = [2]\n",
    "activation = 'relu'\n",
    "algorithm = 'random_hill_climb'\n",
    "max_iters = 1000\n",
    "bias = True\n",
    "is_classifier = True\n",
    "learning_rate = 0.2\n",
    "early_stopping = False\n",
    "max_attempts = 10\n",
    "clip_max = 5\n",
    "\n",
    "nn_modelA1 = mlrose.NeuralNetwork(\n",
    "    hidden_nodes = hidden_nodes,\n",
    "    activation = activation,\n",
    "    algorithm = algorithm,\n",
    "    max_iters = max_iters,\n",
    "    bias = bias,\n",
    "    is_classifier = is_classifier,\n",
    "    learning_rate = learning_rate,\n",
    "    early_stopping = early_stopping,\n",
    "    max_attempts = max_attempts,\n",
    "#     clip_max = clip_max,\n",
    "    random_state = seed_values[4],\n",
    "    curve = True\n",
    ")\n",
    "\n",
    "nn_modelA1.fit(X_train_scaled, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "37b955dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590\n",
      "[[   3.35362094    2.        ]\n",
      " [   3.35360723    4.        ]\n",
      " [   3.35360723    5.        ]\n",
      " ...\n",
      " [   2.30148151 1395.        ]\n",
      " [   2.30141581 1397.        ]\n",
      " [   2.30141581 1398.        ]]\n",
      "2.3014158092452632\n",
      "[[0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.03919367 0.19390826 0.08621977 ... 0.08042162 0.04082504 0.11108355]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "print(len(nn_modelA1.fitted_weights))\n",
    "print(nn_modelA1.fitness_curve)\n",
    "print(nn_modelA1.loss)\n",
    "print(nn_modelA1.predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "56d515b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17691666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict labels for train set and assess accuracy\n",
    "y_train_pred = nn_modelA1.predict(X_train_scaled)\n",
    "\n",
    "y_train_accuracy = accuracy_score(y_train_hot, y_train_pred)\n",
    "print(y_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "00a3c110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0 1 0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[3])\n",
    "print(y_train_pred[3])\n",
    "print(y_train_hot[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d6f91b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_pred[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3ef90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Run for 1 epoch\n",
      "Training Score: 0.10545833333333333\n",
      "Validation Score: 0.10025\n",
      "Training Loss Score: 2.3128669409813685\n",
      "Validation Loss Score: 2.31550835857776\n",
      "Training Time: 0.7898991107940674\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 2 epoch\n",
      "Training Score: 0.10545833333333333\n",
      "Validation Score: 0.10025\n",
      "Training Loss Score: 2.303993712122503\n",
      "Validation Loss Score: 2.3051416775259392\n",
      "Training Time: 0.8366668224334717\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 3 epoch\n",
      "Training Score: 0.7522916666666667\n",
      "Validation Score: 0.748\n",
      "Training Loss Score: 0.851033448222606\n",
      "Validation Loss Score: 0.8867660597885058\n",
      "Training Time: 0.7246658802032471\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 4 epoch\n",
      "Training Score: 0.8608958333333333\n",
      "Validation Score: 0.8515833333333334\n",
      "Training Loss Score: 0.5148386422015143\n",
      "Validation Loss Score: 0.56716265141914\n",
      "Training Time: 0.6933891773223877\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 5 epoch\n",
      "Training Score: 0.9338958333333334\n",
      "Validation Score: 0.9181666666666667\n",
      "Training Loss Score: 0.34035650256869104\n",
      "Validation Loss Score: 0.4022916952377533\n",
      "Training Time: 0.6818728446960449\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 6 epoch\n",
      "Training Score: 0.9636666666666667\n",
      "Validation Score: 0.9490833333333333\n",
      "Training Loss Score: 0.13894551117125234\n",
      "Validation Loss Score: 0.2116431804298949\n",
      "Training Time: 0.7067751884460449\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 7 epoch\n",
      "Training Score: 0.971625\n",
      "Validation Score: 0.9550833333333333\n",
      "Training Loss Score: 0.10764342276936084\n",
      "Validation Loss Score: 0.18822120822112143\n",
      "Training Time: 0.7495579719543457\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 8 epoch\n",
      "Training Score: 0.9765416666666666\n",
      "Validation Score: 0.9595833333333333\n",
      "Training Loss Score: 0.0873542815680298\n",
      "Validation Loss Score: 0.17728788417162647\n",
      "Training Time: 0.7217779159545898\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 9 epoch\n",
      "Training Score: 0.9770208333333333\n",
      "Validation Score: 0.95875\n",
      "Training Loss Score: 0.08074956344695348\n",
      "Validation Loss Score: 0.18525493839530896\n",
      "Training Time: 0.7067670822143555\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 10 epoch\n",
      "Training Score: 0.9782708333333333\n",
      "Validation Score: 0.9585833333333333\n",
      "Training Loss Score: 0.07622326571771143\n",
      "Validation Loss Score: 0.18971498222094313\n",
      "Training Time: 0.6951708793640137\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 11 epoch\n",
      "Training Score: 0.9793125\n",
      "Validation Score: 0.9571666666666667\n",
      "Training Loss Score: 0.07065052838006164\n",
      "Validation Loss Score: 0.18901411398006515\n",
      "Training Time: 0.679912805557251\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 12 epoch\n",
      "Training Score: 0.9822083333333333\n",
      "Validation Score: 0.9608333333333333\n",
      "Training Loss Score: 0.060215809827706544\n",
      "Validation Loss Score: 0.18550934290852952\n",
      "Training Time: 0.7015469074249268\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 13 epoch\n",
      "Training Score: 0.9791458333333334\n",
      "Validation Score: 0.9566666666666667\n",
      "Training Loss Score: 0.07277755374923527\n",
      "Validation Loss Score: 0.2067610652265825\n",
      "Training Time: 0.7946159839630127\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 14 epoch\n",
      "Training Score: 0.9843333333333333\n",
      "Validation Score: 0.95925\n",
      "Training Loss Score: 0.05406088107427706\n",
      "Validation Loss Score: 0.189060243477238\n",
      "Training Time: 0.6861200332641602\n",
      "=============================================\n",
      "=============================================\n",
      "Run for 15 epoch\n",
      "Training Score: 0.983875\n",
      "Validation Score: 0.9585\n",
      "Training Loss Score: 0.05586352043695169\n",
      "Validation Loss Score: 0.20679036265960465\n",
      "Training Time: 0.6997292041778564\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actually training the MLP Classifier to generate the graph for the learning graph visualizer.\n",
    "This should utilize the percentage of samples lists that use randomly selected samples of the\n",
    "overall training data. This is so that we can test the accuracy (using cross-validation) across different\n",
    "training data size samples to see at what point the accuracy score stops being affected by the size of the training\n",
    "samples. This is also to see at what point the data might start getting overfit.\n",
    "\n",
    "This is to generate the Accuracy Learning Curve.\n",
    "\"\"\"\n",
    "# Defining hyperparameters here\n",
    "hidden_layer_sizes = [100, 50, 5]\n",
    "activation = 'relu'\n",
    "learning_rate = 'constant'\n",
    "max_iter = 1 # Setting this to 1 since we want to control the epochs ourselves\n",
    "warm_start = True # This is to stack the training across different epochs\n",
    "\n",
    "number_of_epochs = 15\n",
    "\n",
    "# lists to hold the results of training / validation scores\n",
    "x_axis_list = []\n",
    "avg_train_scores_list = []\n",
    "avg_validation_scores_list = []\n",
    "avg_train_loss_values = []\n",
    "avg_validation_loss_values = []\n",
    "iteration_wall_clock_list = []\n",
    "\n",
    "# First declaring the Decision Tree Classifer from scikit-learn\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    activation=activation,\n",
    "    learning_rate=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    warm_start=warm_start,\n",
    "    random_state=seed_values[0]\n",
    ")\n",
    "\n",
    "for epoch_iteration in range(1, number_of_epochs + 1):\n",
    "    \n",
    "    # cross_val_score doesn't increase across epoch runs for some reason so I need to split it myself\n",
    "    train_data, val_data, train_label, val_label = train_test_split(x_train, y_train, test_size=0.2, random_state=seed_values[0])\n",
    "    start_time = time.time()\n",
    "    clf.fit(train_data, train_label)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    accuracy_score = clf.score(train_data, train_label)\n",
    "    validation_score = clf.score(val_data, val_label)\n",
    "#     loss_score = clf.loss_\n",
    "    train_loss_score = log_loss(train_label, clf.predict_proba(train_data))\n",
    "    val_loss_score = log_loss(val_label, clf.predict_proba(val_data))\n",
    "    \n",
    "    x_axis_list.append(epoch_iteration)\n",
    "    avg_train_scores_list.append(accuracy_score)\n",
    "    avg_validation_scores_list.append(validation_score)\n",
    "    avg_train_loss_values.append(train_loss_score)\n",
    "    avg_validation_loss_values.append(val_loss_score)\n",
    "    iteration_wall_clock_list.append(training_time)\n",
    "    \n",
    "    print(\"=============================================\")\n",
    "    print(\"Run for \" + str(epoch_iteration) + \" epoch\")\n",
    "    print(\"Training Score: \" + str(accuracy_score))\n",
    "    print(\"Validation Score: \" + str(validation_score))\n",
    "#     print(\"Loss Score: \" + str(loss_score))\n",
    "    print(\"Training Loss Score: \" + str(train_loss_score))\n",
    "    print(\"Validation Loss Score: \" + str(val_loss_score))\n",
    "    print(\"Training Time: \" + str(training_time))\n",
    "    print(\"=============================================\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eda52def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "[5 0 4 ... 5 6 8]\n",
      "0.9788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict labels for train set and assess accuracy\n",
    "y_train_pred = clf.predict(x_train)\n",
    "print(y_train_pred)\n",
    "print(y_train)\n",
    "\n",
    "y_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(y_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a96dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
